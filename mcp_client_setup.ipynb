{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "691cafe7",
   "metadata": {},
   "source": [
    "# MCP CLIENT SETUP\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d88bf2df",
   "metadata": {},
   "source": [
    "## 客户端架构\n",
    "- api key，从 llm 平台获取\n",
    "- 核心架构primitives：\n",
    "    1. sampling: 从llm获取回复\n",
    "    2. roots: 客户端制定服务器应重点关注的目录，协调机制\n",
    "    3. elicitation：询问用户信息，征得用户同意\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f022510c",
   "metadata": {},
   "source": [
    "### python 环境设置\n",
    "创建新的python项目\n",
    "```bash\n",
    "uv init mcp-client\n",
    "cd mcp-client\n",
    "```\n",
    "创建新的虚拟环境\n",
    "```bash\n",
    "uv venv\n",
    "source .venv/bin/activate  # macOS/Linux\n",
    ".venv\\Scripts\\activate     # Windows\n",
    "```\n",
    "安装包裹\n",
    "```bash\n",
    "uv add mcp anthropic python-dotenv\n",
    "```\n",
    "Remove the boilerplate files:\n",
    "```bash\n",
    "rm main.py\n",
    "touch client.py\n",
    "```\n",
    "设置llm api key\n",
    "```bash\n",
    "echo \"ANTHROPIC_API_KEY=your-api-key-here\" > .env\n",
    "echo \".env\" >> .gitignore\n",
    "```\n",
    "\n",
    "Node.js 环境设置建新的node项目\n",
    "```bash\n",
    "mkdir mcp-client-typescript\n",
    "cd mcp-client-typescript\n",
    "npm init -y\n",
    "```\n",
    "设置依赖\n",
    "```bash\n",
    "npm install @anthropic-ai/sdk @modelcontextprotocol/sdk dotenv\n",
    "npm install -D @types/node typescript\n",
    "```\n",
    "Create the index.ts file:\n",
    "```bash\n",
    "touch index.ts\n",
    "```\n",
    "更新json 格式\n",
    "```json\n",
    "{\n",
    "  \"type\": \"module\",\n",
    "  \"scripts\": {\n",
    "    \"build\": \"tsc && chmod 755 build/index.js\"\n",
    "  }\n",
    "}\n",
    "```\n",
    "设置环境文件根据API key\n",
    "```bash\n",
    "echo \"ANTHROPIC_API_KEY=your-api-key-here\" > .env\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2c7b1f0",
   "metadata": {},
   "source": [
    "## 基础客户端架构\n",
    "最基本的class设置\n",
    "```python\n",
    "import asyncio\n",
    "from typing import Optional\n",
    "from contextlib import AsyncExitStack\n",
    "\n",
    "from mcp import ClientSession, StdioServerParameters\n",
    "from mcp.client.stdio import stdio_client\n",
    "\n",
    "from 使用的llm模型 import 使用的llm模型\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()  # load environment variables from .env\n",
    "\n",
    "class MCPClient:\n",
    "    def __init__(self):\n",
    "        # Initialize session and client objects\n",
    "        self.session: Optional[ClientSession] = None\n",
    "        self.exit_stack = AsyncExitStack()\n",
    "        self.使用的llm模型 = 使用的llm模型()\n",
    "    # methods will go here\n",
    "```\n",
    "在这之后加入methods ：\n",
    "- `__aenter__` 和 `__aexit__`：用于异步上下文管理，确保会话在使用后被正确关闭。\n",
    "- `connect`：连接到 MCP 服务器，初始化会话。\n",
    "- `disconnect`：关闭会话，释放资源。\n",
    "- `call`：发送 JSON-RPC 请求到服务器，获取响应。\n",
    "\n",
    "**这之后全部跟在def 的下面一层**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27806097",
   "metadata": {},
   "source": [
    "### 连接MCP服务器\n",
    "```python\n",
    "async def connect_to_server(self, server_script_path: str):\n",
    "    \"\"\"Connect to an MCP server\n",
    "\n",
    "    Args:\n",
    "        server_script_path: Path to the server script (.py or .js)\n",
    "    \"\"\"\n",
    "    is_python = server_script_path.endswith('.py')\n",
    "    is_js = server_script_path.endswith('.js')\n",
    "    if not (is_python or is_js):\n",
    "        raise ValueError(\"Server script must be a .py or .js file\")\n",
    "\n",
    "    command = \"python\" if is_python else \"node\"\n",
    "    server_params = StdioServerParameters(\n",
    "        command=command,\n",
    "        args=[server_script_path],\n",
    "        env=None\n",
    "    )\n",
    "\n",
    "    stdio_transport = await self.exit_stack.enter_async_context(stdio_client(server_params))\n",
    "    self.stdio, self.write = stdio_transport\n",
    "    self.session = await self.exit_stack.enter_async_context(ClientSession(self.stdio, self.write))\n",
    "\n",
    "    await self.session.initialize()\n",
    "\n",
    "    # List available tools\n",
    "    response = await self.session.list_tools()\n",
    "    tools = response.tools\n",
    "    print(\"\\nConnected to server with tools:\", [tool.name for tool in tools])\n",
    "\n",
    "    # Store available tools for later use\n",
    "    self.tools = {tool.name: tool for tool in tools}\n",
    "    print(\"Available tools:\", self.tools)\n",
    "    return self.tools\n",
    "    # methods will go here\n",
    "    '''\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea127e5f",
   "metadata": {},
   "source": [
    "### 用户问询逻辑\n",
    "这是客户端的核心部分，处理客户输入以及处理工具调用\n",
    "```python\n",
    "async def process_query(self, query: str) -> str:\n",
    "    \"\"\"Process a query using Claude and available tools\"\"\"\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": query\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    response = await self.session.list_tools()\n",
    "    available_tools = [{\n",
    "        \"name\": tool.name,\n",
    "        \"description\": tool.description,\n",
    "        \"input_schema\": tool.inputSchema\n",
    "    } for tool in response.tools]\n",
    "\n",
    "    # Initial Claude API call\n",
    "    response = self.anthropic.messages.create(\n",
    "        model=\"claude-sonnet-4-20250514\",\n",
    "        max_tokens=1000,\n",
    "        messages=messages,\n",
    "        tools=available_tools\n",
    "    )\n",
    "\n",
    "    # Process response and handle tool calls\n",
    "    final_text = []\n",
    "\n",
    "    assistant_message_content = []\n",
    "    for content in response.content:\n",
    "        if content.type == 'text':\n",
    "            final_text.append(content.text)\n",
    "            assistant_message_content.append(content)\n",
    "        elif content.type == 'tool_use':\n",
    "            tool_name = content.name\n",
    "            tool_args = content.input\n",
    "\n",
    "            # Execute tool call\n",
    "            result = await self.session.call_tool(tool_name, tool_args)\n",
    "            final_text.append(f\"[Calling tool {tool_name} with args {tool_args}]\")\n",
    "\n",
    "            assistant_message_content.append(content)\n",
    "            messages.append({\n",
    "                \"role\": \"assistant\",\n",
    "                \"content\": assistant_message_content\n",
    "            })\n",
    "            messages.append({\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [\n",
    "                    {\n",
    "                        \"type\": \"tool_result\",\n",
    "                        \"tool_use_id\": content.id,\n",
    "                        \"content\": result.content\n",
    "                    }\n",
    "                ]\n",
    "            })\n",
    "\n",
    "            # Get next response from Claude\n",
    "            response = self.anthropic.messages.create(\n",
    "                model=\"claude-sonnet-4-20250514\",\n",
    "                max_tokens=1000,\n",
    "                messages=messages,\n",
    "                tools=available_tools\n",
    "            )\n",
    "\n",
    "            final_text.append(response.content[0].text)\n",
    "\n",
    "    return \"\\n\".join(final_text)\n",
    "    '''\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "144f9181",
   "metadata": {},
   "source": [
    "### 交互式界面\n",
    "建立对话轮回，清理功能\n",
    "```python\n",
    "async def chat_loop(self):\n",
    "    \"\"\"Run an interactive chat loop\"\"\"\n",
    "    print(\"\\nMCP Client Started!\")\n",
    "    print(\"Type your queries or 'quit' to exit.\")\n",
    "\n",
    "    while True:\n",
    "        try:\n",
    "            query = input(\"\\nQuery: \").strip()\n",
    "\n",
    "            if query.lower() == 'quit':\n",
    "                break\n",
    "\n",
    "            response = await self.process_query(query)\n",
    "            print(\"\\n\" + response)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"\\nError: {str(e)}\")\n",
    "\n",
    "async def cleanup(self):\n",
    "    \"\"\"Clean up resources\"\"\"\n",
    "    await self.exit_stack.aclose()\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed80011b",
   "metadata": {},
   "source": [
    "### 接入点，主要执行逻辑\n",
    "```python\n",
    "async def main():\n",
    "    if len(sys.argv) < 2:\n",
    "        print(\"Usage: python client.py <path_to_server_script>\")\n",
    "        sys.exit(1)\n",
    "\n",
    "    client = MCPClient()\n",
    "    try:\n",
    "        await client.connect_to_server(sys.argv[1])\n",
    "        await client.chat_loop()\n",
    "    finally:\n",
    "        await client.cleanup()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    import sys\n",
    "    asyncio.run(main())\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05616afe",
   "metadata": {},
   "source": [
    "## 完善思路\n",
    "1. 加强工具调用处理，在‘process_query’中增加工具调用处理,错误处理\n",
    "2. 回复处理，设置回复格式\n",
    "3. 用户界面\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b504c7ad",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
